{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Functional Connectivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers:\n",
    "\n",
    "* How to download and load COBRE resting state functional datasets.\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "* How to measure connectomes from brain data decomposed to functionally defined regions.\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "* SVM based classification of healthy subjects and schizophrenic datasets\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All methods are adapted from Nilearn, a Python based toolbox developed for resting state analysis of Brain data. Nilearn is heavily dependent on scikit-learn machine learning library http://scikit-learn.org/stable/, numpy, matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing pipeline example for resting state fMRI datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "Fetch COBRE datasets shipped with Nilearn\n",
    "-----------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COBRE datasets release version 0.17 preprocessed using NIAK pipeline. This\n",
    "# release consists of light weight data with standard preprocessing\n",
    "# steps used in functional MRI setting.\n",
    "\n",
    "from nilearn import datasets\n",
    "\n",
    "# all subjects (146 given in n_subjects argument)\n",
    "cobre_data = datasets.fetch_cobre(n_subjects=146)\n",
    "print(cobre_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path to functional datasets for each subject\n",
    "func_imgs = cobre_data.func\n",
    "print(\"Paths to data are downloaded and stored in folder \"\n",
    "      \"'nilearn_data' in home directory:{0}\"\n",
    "      .format(func_imgs[0]))\n",
    "print(\"=============================================\")\n",
    "\n",
    "# Path to confounds: motion, slow drift, compcorr\n",
    "confounds = cobre_data.confounds\n",
    "print(\"Paths to confounds for each subject are also at \"\n",
    "      \"stored same location \"\n",
    "      \"as functional data:{0}\".format(confounds[0]))\n",
    "print(\"===============================================\")\n",
    "\n",
    "# Clinical variables are returned as Numpy array but not a path\n",
    "phenotypes = cobre_data.phenotypic\n",
    "print(phenotypes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Brain Regions of Interest (ROI)\n",
    "---------------------------------------\n",
    "We use pre-generated atlas (BASC) to parcellate brain into ROIs. For this, we simply fetch from Nilearn\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basc_atlases = datasets.fetch_atlas_basc_multiscale_2015()\n",
    "\n",
    "# Now, we have atlases of all scales (n=7, 12, 20, 36, 64,\n",
    "# 122, 197, 325, 444) in the networks.\n",
    "\n",
    "atlas_img = basc_atlases['scale122']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: Labelling based brain atlas\n",
    "------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "plotting.plot_roi(atlas_img, cmap='Paired', colorbar=True,\n",
    "                  title='BASC atlas of 122 networks denoted as labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Timeseries Extraction\n",
    "-----------------------------\n",
    "Extract subject specific timeseries signals from brain atlas\n",
    "--------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For timeseries extraction from functional images, we import\n",
    "# `NiftiLabelsMasker` from input_data module.\n",
    "\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "# We initialize the timeseries extractor by setting standard processing\n",
    "# parameters required for the extraction.\n",
    "\n",
    "timeseries_extractor = NiftiLabelsMasker(\n",
    "    labels_img=atlas_img,  # brain atlas\n",
    "    smoothing_fwhm=6.,  # Smoothing\n",
    "    standardize=True, detrend=True, # timeseries signals\n",
    "    memory='nilearn_cache',  # joblib\n",
    "    memory_level=2,  # Level of caching\n",
    "    verbose=2)  # useful to see processing\n",
    "\n",
    "# We call `fit_transform` on each subject fMRI data.\n",
    "\n",
    "# `fit` will prepare 2D data matrix from 4D functional images, whereas\n",
    "# `transform` will filter, extracts and gives out cleaned signals.\n",
    "# Filtering can be done based on parameters given while initialization\n",
    "# and from given confounds as given in below argument.\n",
    "\n",
    "subjects_timeseries = []\n",
    "\n",
    "for func_img in cobre_data.func:\n",
    "    signals = timeseries_extractor.fit_transform(func_img)\n",
    "    subjects_timeseries.append(signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: Extracted timeseries signals (before confounds)\n",
    "--------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We visualize timeseries signals by importing matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We show single subject timeseries signals extracted without confounds\n",
    "plt.figure()\n",
    "plt.plot(subjects_timeseries[2])\n",
    "plt.title('Timeseries for single subject extracted from 122 brain regions')\n",
    "plt.xlabel('Number of regions')\n",
    "plt.ylabel('Normalized signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries extraction with confounds\n",
    "------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects_timeseries_with_confounds = []\n",
    "\n",
    "for func_img, confound in zip(func_imgs, confounds):\n",
    "    signals = timeseries_extractor.fit_transform(func_img,\n",
    "                                                 confounds=confound)\n",
    "    subjects_timeseries_with_confounds.append(signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: Extracting timeseries signals (after confounds)\n",
    "--------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use same matplotlib.pyplot to plot the signals\n",
    "plt.figure()\n",
    "plt.plot(subjects_timeseries_with_confounds[2])\n",
    "plt.title('Timeseries signal (after removing confounds)')\n",
    "plt.xlabel('Number of regions')\n",
    "plt.ylabel('Normalized signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Connectomes estimation\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build connectivity matrices using simple 'correlation' on extracted timeseries\n",
    "# signals\n",
    "\n",
    "# We import `ConnectivityMeasure` from connectome module and covariance\n",
    "# estimator LedoitWolf from scikit learn to build connectivity matrices.\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "# Initialize `ConnectivityMeasure` and measure set in argument 'kind' and\n",
    "# argument 'cov_estimator'\n",
    "connectivity = ConnectivityMeasure(kind='correlation',\n",
    "                                   cov_estimator=LedoitWolf(assume_centered=True))\n",
    "connectivity_matrices = connectivity.fit_transform(subjects_timeseries_with_confounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: Mean connectome matrix with measure = 'correlation'\n",
    "------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we get the mean of the correlation matrix\n",
    "mean_correlation_matrix = connectivity_matrices.mean(axis=0)\n",
    "\n",
    "# Second, visualizing goes here using matplotlib\n",
    "\n",
    "title = ('Mean over all connectivity matrices with measure \"correlation\"')\n",
    "plt.figure()\n",
    "plt.title(title)\n",
    "plt.imshow(mean_correlation_matrix, cmap='Paired')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using Support Vector Classifier\n",
    "------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Till now, we showed you how to measure connectomes using 'correlation'\n",
    "# measure. Now, we show how to convert connectivity matrices estimated on each\n",
    "# subject to vector (which contains lower traingular part of those matrices).\n",
    "# Use these vector coefficients to classify between schizophrenia and\n",
    "# controls.\n",
    "\n",
    "# Our classification is based on Support Vector Machine linear classifier\n",
    "# and using Stratified Shuffle Split cross validation where we train the\n",
    "# classifier by 75% of the train size and test on the remaining 25%. We repeat\n",
    "# the same with 100 random splits.\n",
    "\n",
    "# In this setting, we will be heavily be dependent on scikit-learn library\n",
    "# which has cool and easy to use machine learning classifiers useful for\n",
    "# Neuroimaging community.\n",
    "\n",
    "# For prediction, we convert connectivity matrices to vector, define cross\n",
    "# validator and use cross_val_score to predict scores for 100 splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric matrices to vector\n",
    "----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.connectome import sym_to_vec\n",
    "\n",
    "connectivity_coefs = sym_to_vec(connectivity_matrices)\n",
    "# Now, we have connectivity coefficients of all subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validator - for Prediction\n",
    "---------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "n_iter = 100  # Number of splits\n",
    "class_type = 'subject_type'  # from phenotypes in cobre data return\n",
    "classes = np.asarray(phenotypes[class_type])\n",
    "_, classes_binary = np.unique(classes, return_inverse=True)\n",
    "cv = StratifiedShuffleSplit(n_splits=n_iter,\n",
    "                            test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we predict using SVC\n",
    "-------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC(penalty='l2', random_state=0)\n",
    "iter_for_prediction = cv.split(func_imgs, classes_binary)\n",
    "scores = []\n",
    "# Try cv.get_n_splits()\n",
    "for index, (train_index, test_index) in enumerate(iter_for_prediction):\n",
    "    prediction_scores = cross_val_score(estimator=svc,  # classifier\n",
    "                                        X=connectivity_coefs,  # Data to fit\n",
    "                                        y=classes_binary,  # Target variables\n",
    "                                        scoring='roc_auc',  # scoring\n",
    "                                        cv=[(train_index, test_index)],\n",
    "                                        )\n",
    "    scores.append(prediction_scores)  # for each split we gather scores\n",
    "\n",
    "scores = np.asarray(scores)\n",
    "print(\" -- Support Vector Classification -- \")\n",
    "print(\"Classification scores '%s': %1.2f +/- %1.2f\" % ('correlation',\n",
    "                                                       scores.mean(),\n",
    "                                                       scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing connectomes using another Tangent Embedding\n",
    "------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We try to improve classification accuracy by using different measure\n",
    "# We repeat the same steps as above from Step 3 but using now kind='tangent' to\n",
    "# see if we can improve our classification accuracy\n",
    "connectivity = ConnectivityMeasure(kind='tangent',\n",
    "                                   cov_estimator=LedoitWolf(assume_centered=True))\n",
    "# Compute connectome matrices\n",
    "connectivity_matrices = connectivity.fit_transform(subjects_timeseries_with_confounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization: Mean connectome matrix with measure = 'tangent'\n",
    "---------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we get the mean of the tangent matrix from attribute 'mean_'\n",
    "mean_tangent_matrix = connectivity.mean_\n",
    "\n",
    "# Second, visualizing goes here using matplotlib\n",
    "\n",
    "title = ('Mean over all connectivity matrices with measure \"tangent\"')\n",
    "plt.figure()\n",
    "plt.title(title)\n",
    "plt.imshow(mean_tangent_matrix, cmap='Paired')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction using Tangent space\n",
    "------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert connectivity matrices to connectivity vectors\n",
    "connectivity_coefs2 = sym_to_vec(connectivity_matrices)\n",
    "\n",
    "# Use them again for classification by using the same cross validator\n",
    "iter_for_prediction2 = cv.split(func_imgs, classes_binary)\n",
    "scores2 = []\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(iter_for_prediction2):\n",
    "    prediction_scores2 = cross_val_score(estimator=svc,  # classifier\n",
    "                                         X=connectivity_coefs2,  # Data to fit\n",
    "                                         y=classes_binary,  # Target variables\n",
    "                                         scoring='roc_auc',  # scoring\n",
    "                                         cv=[(train_index, test_index)],\n",
    "                                         )\n",
    "    scores2.append(prediction_scores2)  # for each split we gather scores\n",
    "\n",
    "scores2 = np.asarray(scores2)\n",
    "print(\" -- Support Vector Classification with Tangent Space measure -- \")\n",
    "print(\"Classification scores '%s': %1.2f +/- %1.2f\" % ('tangent',\n",
    "                                                       scores2.mean(),\n",
    "                                                       scores2.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
